{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, Model, models\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, Flatten, Dense, Reshape, Conv2DTranspose, Activation\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Flatten, Dense, Reshape\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pdb\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.optimizers import SGD,RMSprop\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained models\n",
    "signal_model = load_model(r'D:\\SDRChallenge-main\\SDR_data\\ANN_tuned.h5')\n",
    "scalogram_model = load_model(r'D:\\SDRChallenge-main\\SDR_data\\Model_spectograms.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze pretrained models\n",
    "signal_model.trainable = False\n",
    "scalogram_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove top layers of both models\n",
    "signal_feature_extractor = Model(inputs=signal_model.input, outputs=signal_model.layers[-5].output)\n",
    "scalogram_feature_extractor = Model(inputs=scalogram_model.input, outputs=scalogram_model.layers[-2].output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_feature_extractor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalogram_feature_extractor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define inputs for the unified model\n",
    "signal_input = Input(shape=(614400,), name='signal_input')  # Adjust shape if needed\n",
    "scalogram_input = Input(shape=(256, 256, 3), name='scalogram_input')  # Shape matches scalogram images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features\n",
    "signal_features = signal_feature_extractor(signal_input)\n",
    "scalogram_features = scalogram_feature_extractor(scalogram_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Concatenate, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_features = Concatenate()([signal_features, scalogram_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ANN layers for classification\n",
    "x = Dense(128, activation='relu')(combined_features)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(4, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the unified model\n",
    "unified_model = Model(inputs=[signal_input, scalogram_input], outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "# Compile the model with a low learning rate\n",
    "unified_model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-5),  # Start with a small learning rate\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path= r\"D:\\SDRChallenge-main\\SDR_data\\I_Q_data.Numpy\\SNR0\"\n",
    "\n",
    "def load_npy_data(folder_path, label):\n",
    "    data_list = []\n",
    "    #c = 0  \n",
    "    for subdir, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(\".npy\"):\n",
    "                file_path = os.path.join(subdir, file)\n",
    "                data = np.load(file_path)  # Load the .npy file\n",
    "                # print(data.shape)\n",
    "                data = np.nan_to_num(data, nan=0.0) # Keep rows with no NaN value\n",
    "                # print(data.shape)\n",
    "\n",
    "                # Assuming data has I values in the first column and Q values in the second column\n",
    "                I_values = data[:, 0]\n",
    "                Q_values = data[:, 1]\n",
    "                stacked_IQ_values = np.concatenate((I_values, Q_values), axis=0).reshape(-1, 1)\n",
    "                # print(stacked_IQ_values.shape)\n",
    "                transposed_IQ_values = stacked_IQ_values.T\n",
    "                # print(transposed_IQ_values.shape)\n",
    "                labeled_data = np.append(transposed_IQ_values, label).reshape(1, -1)\n",
    "                              \n",
    "                # Check for NaN or Inf values and handle them\n",
    "                # if np.isnan(data).any() or np.isinf(data).any():\n",
    "                #     data = np.nan_to_num(data)  # Replace NaN and Inf with 0\n",
    "                # # Add labeled row to the data list\n",
    "                data_list.append(labeled_data)\n",
    "                # print(labeled_data.shape)\n",
    "                # print(len(data_list))\n",
    "    return np.vstack(data_list) if data_list else None\n",
    "\n",
    "\n",
    "radar_data = load_npy_data(r'D:\\SDRChallenge-main\\SDR_data\\I_Q_data.Numpy\\SNR0\\Radar', label=0)\n",
    "g5_data = load_npy_data(r'D:\\SDRChallenge-main\\SDR_data\\I_Q_data.Numpy\\SNR0\\5G_Only', label=1)\n",
    "radar_5g_data = load_npy_data(r'D:\\SDRChallenge-main\\SDR_data\\I_Q_data.Numpy\\SNR0\\5G+Radar', label=2)\n",
    "Noise = load_npy_data(r'D:\\SDRChallenge-main\\SDR_data\\I_Q_data.Numpy\\SNR0\\Noise', label=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine data if available\n",
    "data_to_combine = [d for d in [radar_data, g5_data, radar_5g_data, Noise] if d is not None]\n",
    "# data_to_combine = [d for d in [radar_data, g5_data] if d is not None]\n",
    "print(len(data_to_combine))\n",
    "if data_to_combine:\n",
    "    combined_data = np.vstack(data_to_combine)\n",
    "    # Print shape of the combined data\n",
    "    print(f\"Combined data shape: {combined_data.shape}\")\n",
    "    # Display first few rows to verify the format\n",
    "    print(\"First few rows of combined data:\")\n",
    "    print(combined_data[:5])\n",
    "else:\n",
    "    print(\"No data loaded from the specified folders.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = combined_data[:, :-1]  # Features (all columns except the last)\n",
    "y = combined_data[:, -1]   # Labels (last column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y, num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 2: Split the training data into training and validation sets (e.g., 70% train, 10% validation)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.125, random_state=42)\n",
    "\n",
    "# # Print the sizes of each set\n",
    "# print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "# print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "# print(f\"Test set: {X_test.shape[0]} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'D:\\SDRChallenge-main\\SDR_data\\SNR0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "img_height = 256\n",
    "img_width = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    path,\n",
    "    validation_split=0.2, # 20% of the data for validation\n",
    "    subset=\"training\",    # 80% of the data for training\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    label_mode='categorical',    \n",
    ")\n",
    "# Load validation/testing dataset (remaining 30% of the data)\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    path,\n",
    "    validation_split=0.2, # 20% for validation/testing\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    label_mode='categorical',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_ds.class_names\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# train_ds = augmented_train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "# val_ds = augmented_val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "# test_ds = augmented_test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "# test_ds= test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "# test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_signal_and_scalogram(signal_path, scalogram_path, label):\n",
    "    signal_files = {os.path.splitext(f)[0]: f for f in os.listdir(signal_path) if f.endswith('.npy')}\n",
    "    scalogram_files = {os.path.splitext(f)[0]: f for f in os.listdir(scalogram_path) if f.endswith('.png')}\n",
    "    \n",
    "    matched_data = []\n",
    "    \n",
    "    for name in signal_files.keys():\n",
    "        if name in scalogram_files:\n",
    "            # Load signal\n",
    "            signal = np.load(os.path.join(signal_path, signal_files[name]))\n",
    "            signal = np.nan_to_num(signal, nan=0.0)\n",
    "\n",
    "            # Load scalogram\n",
    "            scalogram = tf.keras.utils.load_img(\n",
    "                os.path.join(scalogram_path, scalogram_files[name]),\n",
    "                target_size=(256, 256)  # Resize to desired dimensions\n",
    "            )\n",
    "            scalogram = tf.keras.utils.img_to_array(scalogram) / 255.0  # Normalize to [0, 1]\n",
    "            \n",
    "            # Append (signal, scalogram, label)\n",
    "            matched_data.append((signal, scalogram, label))\n",
    "        else:\n",
    "            print(f\"Scalogram not found for signal: {name}.npy\")\n",
    "    \n",
    "    return matched_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_path = r'D:\\SDRChallenge-main\\SDR_data\\I_Q_data.Numpy\\SNR_10'\n",
    "scalogram_path = r'D:\\SDRChallenge-main\\SDR_data\\SNR-10'\n",
    "\n",
    "radar_data = load_signal_and_scalogram(signal_path, scalogram_path, label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = {\n",
    "    'Radar': 0,\n",
    "    '5G_Only': 1,\n",
    "    '5G+Radar': 2,\n",
    "    'Noise': 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_paired_dataset_for_class(signal_base_path, scalogram_base_path, class_name, label):\n",
    "    signal_path = os.path.join(signal_base_path, class_name)\n",
    "    scalogram_path = os.path.join(scalogram_base_path, class_name)\n",
    "    \n",
    "    # Get filenames without extensions\n",
    "    signal_files = {os.path.splitext(f)[0]: f for f in os.listdir(signal_path) if f.endswith('.npy')}\n",
    "    scalogram_files = {os.path.splitext(f)[0]: f for f in os.listdir(scalogram_path) if f.endswith('.png')}\n",
    "    \n",
    "    paired_data = []\n",
    "    \n",
    "    for name in signal_files.keys():\n",
    "        if name in scalogram_files:\n",
    "            # Load signal\n",
    "            signal = np.load(os.path.join(signal_path, signal_files[name]))\n",
    "            signal = np.nan_to_num(signal, nan=0.0)\n",
    "\n",
    "            # Load scalogram\n",
    "            scalogram = tf.keras.utils.load_img(\n",
    "                os.path.join(scalogram_path, scalogram_files[name]),\n",
    "                target_size=(256, 256)\n",
    "            )\n",
    "            scalogram = tf.keras.utils.img_to_array(scalogram) / 255.0  # Normalize\n",
    "            \n",
    "            # Append (signal, scalogram, label)\n",
    "            paired_data.append((signal, scalogram, label))\n",
    "        else:\n",
    "            print(f\"Unmatched file in class '{class_name}': {name}\")\n",
    "    \n",
    "    return paired_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_full_dataset(signal_base_path, scalogram_base_path, class_labels):\n",
    "    full_data = []\n",
    "    for class_name, label in class_labels.items():\n",
    "        class_data = create_paired_dataset_for_class(signal_base_path, scalogram_base_path, class_name, label)\n",
    "        full_data.extend(class_data)\n",
    "        print(f\"Class '{class_name}' has {len(class_data)} paired samples.\")\n",
    "    return full_data\n",
    "\n",
    "# Paths to the signal and scalogram directories\n",
    "signal_base_path = r'C:\\Users\\KhanShafiUllah\\OneDrive - UT Arlington\\SNR_10'\n",
    "scalogram_base_path = r'C:\\Users\\KhanShafiUllah\\OneDrive - UT Arlington\\SNR-10'\n",
    "\n",
    "# Create dataset\n",
    "all_data = create_full_dataset(signal_base_path, scalogram_base_path, class_labels)\n",
    "\n",
    "print(f\"Total paired samples: {len(all_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate signals, scalograms, and labels\n",
    "signals = np.array([item[0] for item in all_data])\n",
    "scalograms = np.array([item[1] for item in all_data])\n",
    "labels = np.array([item[2] for item in all_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode labels\n",
    "labels = tf.keras.utils.to_categorical(labels, num_classes=len(class_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and validation sets\n",
    "X_train_signal, X_val_signal, X_train_scalogram, X_val_scalogram, y_train, y_val = train_test_split(\n",
    "    signals, scalograms, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {X_train_signal.shape[0]}\")\n",
    "print(f\"Validation samples: {X_val_signal.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_signal = scaler.fit_transform(X_train_signal)\n",
    "X_train_signal = scaler.transform(X_val_signal)\n",
    "\n",
    "# Normalize scalograms (if not already normalized)\n",
    "X_train_scalogram = X_train_scalogram / 255.0\n",
    "X_val_scalogram = X_val_scalogram / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = unified_model.fit(\n",
    "    {'signal_input': X_train_signal, 'scalogram_input': X_train_scalogram},\n",
    "    y_train,\n",
    "    validation_data=(\n",
    "        {'signal_input': X_train_signal, 'scalogram_input': X_val_scalogram},\n",
    "        y_val\n",
    "    ),\n",
    "    epochs=10,\n",
    "    batch_size=16\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF_GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
